{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics for Robots 101\n",
    "\n",
    "... lets make robots ethical!\n",
    "\n",
    "\n",
    "Idea of this task is to use a foundation model and enrich it with an ethics dataset without adjusting the existing weights of the pre-trained model.\n",
    "\n",
    "This concept is referred to as \"Parameter Efficient Fine Tuning (PEFT)\" (we will be using LoRA in partiular). The goal of this task is to get familiar with the LoRA method (but we will try to make it a bit more intersting).\n",
    "\n",
    "\n",
    "\n",
    "## LoRA explained\n",
    "Low-Rank Adaptation (LoRA) is a transfer learning method that uses a pre-trained model and extends it by an additional layer. The output of the additional layer is combined with an existing layer output.\n",
    "This enables to utilize the pre-trained weights of an existing model and extend the model with additional trainind data for a (domain specific) task.\n",
    "\n",
    "Relevant publication: LoRA: Low-Rank Adaptation of Large Language Models, Hu et. al., 2021, https://arxiv.org/abs/2106.09685\n",
    "\n",
    "\n",
    "## Dataset explained\n",
    "\n",
    "We will use the ETHICS dataset representing set of binary labeled ethical / unethical scenarios. The purpose of this dataset is to teach AI models \"basic concepts of morality\" (refer publication below).The scenarios are categorized into several sets as \"commonsense\", \"justice\", etc.\n",
    "\n",
    "An example of an unethical scenario from the \"commonsense\" category: \"I talked loudly during the movie I was watching at the crowded movie theater.\"\n",
    "\n",
    "=> Note that we would interpret an ethical scenario as positive and an unethincal as negative for the sentiment classificaion task below.\n",
    "\n",
    "The dataset can be downloaded from https://huggingface.co/datasets/hendrycks/ethics.\n",
    "\n",
    "Relevant publication: Aligning AI With Shared Human Values, Hendrycks et. al., 2021, https://arxiv.org/pdf/2008.02275.pdf\n",
    "\n",
    "\n",
    "## Model Explained\n",
    "\n",
    "The model used is an adpation (reduction) based on the BERT base model trained on Wikipeda and the \"Bookcorpus\" dataset (https://huggingface.co/datasets/bookcorpus). \n",
    "The reduced size as well as the additional training on a book dataset promise interesting results for this task.\n",
    "\n",
    "Model page on huggingface: https://huggingface.co/distilbert/distilbert-base-uncased\n",
    "\n",
    "Relevant publication: Sanh et. al., 2002, https://arxiv.org/abs/1910.01108\n",
    "\n",
    "\n",
    "## Task Overview\n",
    "\n",
    "We will train and evaluate two different (sentiment) classifiers. All of wich will be based on the \"distilbert-based-uncased\" model.\n",
    "\n",
    "1. \"Reference Classifier\": foundation model trained on the ETHICS dataset\n",
    "2. \"LoRA Classifier\": foundation model fine tuned on the ETHICS dataset using LoRA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model path\n",
    "PRE_TRAINED_MODEL = \"distilbert-base-uncased\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'input'],\n",
      "        num_rows: 13910\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['label', 'input'],\n",
      "        num_rows: 3885\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['label', 'input'],\n",
      "        num_rows: 3964\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load ethics_dataset\n",
    "ethics_dataset = load_dataset(\"hendrycks/ethics\", trust_remote_code=True)\n",
    "print(ethics_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Process Dataset\n",
    "\n",
    "The data might be is unfortunately not 100% compatible with the trained model.\n",
    "While most sentiment outputs use 0 for negative and 1 for positive sentiment, the ETHICS dataset uses 0 for \"acceptable\" and 1 for \"not acceptable\" scenarios. In order to match the model and the data, we need to invert the labels of the dataset.\n",
    "This way we would represent a positive sentiment with an ethically acceptable behavior and a negative sentiment with an unethical behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 0, 'input': 'I exposed the truth about their employee spreading fake news.'}\n",
      "{'label': 1, 'input': 'I exposed the truth about their employee spreading fake news.'}\n"
     ]
    }
   ],
   "source": [
    "# define function for label swap\n",
    "def swap_labels(item):\n",
    "    current_label = item[\"label\"]\n",
    "    if current_label == 0:\n",
    "       item[\"label\"] = 1\n",
    "    elif current_label == 1:\n",
    "       item[\"label\"] = 0\n",
    "    else:\n",
    "        print(\"YOU HAVE MORE LABELS THAN YOU ARE SUPPOSED TO HAVE\")\n",
    "    return item\n",
    "\n",
    "print(ethics_dataset[\"test\"][0])\n",
    "# run label swap on datasets\n",
    "ethics_dataset[\"test\"] = ethics_dataset[\"test\"].map(swap_labels)\n",
    "ethics_dataset[\"train\"] = ethics_dataset[\"train\"].map(swap_labels)\n",
    "print(ethics_dataset[\"test\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement some useful helpers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-06 12:37:49.495895: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-06 12:37:50.453550: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "\n",
    "def create_trainer(model, directory, train_data, test_data):\n",
    "    return Trainer(\n",
    "            model = model,\n",
    "            args = TrainingArguments(\n",
    "                output_dir = directory,\n",
    "                #optim = \"adamw_bnb_8bit\", # use quantization in optimizer (speeding up training)\n",
    "                per_device_train_batch_size = 2,\n",
    "                per_device_eval_batch_size = 2,\n",
    "                evaluation_strategy = \"epoch\",\n",
    "                save_strategy = \"epoch\",\n",
    "                num_train_epochs = 2,\n",
    "                load_best_model_at_end = True,\n",
    "            ),\n",
    "            train_dataset = train_data, # tokenized_dataset[\"train\"],\n",
    "            eval_dataset =  test_data, #tokenized_dataset[\"test\"],\n",
    "            tokenizer = tokenizer,\n",
    "            data_collator = DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "            compute_metrics=compute_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define metric computation\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label={0: \"Negative\", 1: \"Positive\"} \n",
    "label2id={\"Negative\": 0, \"Positive\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_text(input, classifier):\n",
    "    classifier.to('cuda')\n",
    "    # tokenize inputs\n",
    "    inputs = tokenizer(input, truncation=True, padding=True, return_tensors=\"pt\").input_ids.to('cuda')\n",
    "    # get logits of classifier\n",
    "    outputs = lora_classifier(inputs).logits\n",
    "    # apply softmax\n",
    "    probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "    # get predicted class\n",
    "    predicted_class = torch.argmax(probabilities)\n",
    "    #print result\n",
    "    \n",
    "    if predicted_class == 1:\n",
    "        print(\"Positive scenario \" + str(probabilities[0][1] * 100))\n",
    "    else:\n",
    "        print(\"Negaive scenario \" + str(probabilities[0][0] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(dataset, tokenizer, content_column):\n",
    "    # tokenize dataset\n",
    "    tokenized_dataset = {}\n",
    "    for item in dataset:\n",
    "        tokenized_dataset[item] = dataset[item].map(\n",
    "            lambda x: tokenizer(x[content_column], truncation=True), batched=True\n",
    "        )\n",
    "    return tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from transformers.utils import logging\n",
    "logging.set_verbosity_error() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a dictionary that contains the train, test and validation data. The contents are located in the \"input\" column and require to be tokenized (split into the tokens that were learned by the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': Dataset({\n",
      "    features: ['label', 'input', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 13910\n",
      "}), 'validation': Dataset({\n",
      "    features: ['label', 'input', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 3885\n",
      "}), 'test': Dataset({\n",
      "    features: ['label', 'input', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 3964\n",
      "})}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRE_TRAINED_MODEL) # use tokens from model\n",
    "\n",
    "tokenized_ethics_dataset = tokenize_dataset(ethics_dataset, tokenizer, \"input\")\n",
    "\n",
    "print(tokenized_ethics_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only subset of train / test data (due to limited computational resources available)\n",
    "num_train = 2000\n",
    "num_test =  500\n",
    "\n",
    "train_data_ethics = tokenized_ethics_dataset[\"train\"].shuffle(seed=42).select(range(num_train))\n",
    "test_data_ethics = tokenized_ethics_dataset[\"test\"].shuffle(seed=42).select(range(num_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Reference Classifier\n",
    "\n",
    "In order to compare the results of fine tuning, we will train a reference classifier by adding a new head onto an pre-trained model and train the particular head only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "\n",
    "reference_classifier = BertForSequenceClassification.from_pretrained(PRE_TRAINED_MODEL,\n",
    "                                                                     num_labels = 2,\n",
    "                                                                     label2id = label2id,\n",
    "                                                                     id2label = id2label)\n",
    "\n",
    "# freeze existing model weights (make sure you are not updating the pre-trained model)\n",
    "for parameter in reference_classifier.base_model.parameters():\n",
    "    parameter.reuires_grad = False\n",
    "\n",
    "reference_classifier.to(\"cuda\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "reference_classifier_trainer = create_trainer(reference_classifier, \"data/reference_classifier_\",\n",
    "                                              train_data_ethics, test_data_ethics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempted to log scalar metric loss:\n",
      "0.742\n",
      "Attempted to log scalar metric grad_norm:\n",
      "6.092630386352539\n",
      "Attempted to log scalar metric learning_rate:\n",
      "3.7500000000000003e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "0.5\n",
      "{'loss': 0.742, 'grad_norm': 6.092630386352539, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.5}\n",
      "Attempted to log scalar metric loss:\n",
      "0.7207\n",
      "Attempted to log scalar metric grad_norm:\n",
      "3.4796862602233887\n",
      "Attempted to log scalar metric learning_rate:\n",
      "2.5e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "1.0\n",
      "{'loss': 0.7207, 'grad_norm': 3.4796862602233887, 'learning_rate': 2.5e-05, 'epoch': 1.0}\n",
      "Attempted to log scalar metric eval_loss:\n",
      "0.6946001052856445\n",
      "Attempted to log scalar metric eval_accuracy:\n",
      "0.504\n",
      "Attempted to log scalar metric eval_runtime:\n",
      "8.1391\n",
      "Attempted to log scalar metric eval_samples_per_second:\n",
      "61.432\n",
      "Attempted to log scalar metric eval_steps_per_second:\n",
      "30.716\n",
      "Attempted to log scalar metric epoch:\n",
      "1.0\n",
      "{'eval_loss': 0.6946001052856445, 'eval_accuracy': 0.504, 'eval_runtime': 8.1391, 'eval_samples_per_second': 61.432, 'eval_steps_per_second': 30.716, 'epoch': 1.0}\n",
      "Attempted to log scalar metric loss:\n",
      "0.7078\n",
      "Attempted to log scalar metric grad_norm:\n",
      "13.974932670593262\n",
      "Attempted to log scalar metric learning_rate:\n",
      "1.25e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "1.5\n",
      "{'loss': 0.7078, 'grad_norm': 13.974932670593262, 'learning_rate': 1.25e-05, 'epoch': 1.5}\n",
      "Attempted to log scalar metric loss:\n",
      "0.6936\n",
      "Attempted to log scalar metric grad_norm:\n",
      "3.538149833679199\n",
      "Attempted to log scalar metric learning_rate:\n",
      "0.0\n",
      "Attempted to log scalar metric epoch:\n",
      "2.0\n",
      "{'loss': 0.6936, 'grad_norm': 3.538149833679199, 'learning_rate': 0.0, 'epoch': 2.0}\n",
      "Attempted to log scalar metric eval_loss:\n",
      "0.6944364309310913\n",
      "Attempted to log scalar metric eval_accuracy:\n",
      "0.504\n",
      "Attempted to log scalar metric eval_runtime:\n",
      "7.9074\n",
      "Attempted to log scalar metric eval_samples_per_second:\n",
      "63.232\n",
      "Attempted to log scalar metric eval_steps_per_second:\n",
      "31.616\n",
      "Attempted to log scalar metric epoch:\n",
      "2.0\n",
      "{'eval_loss': 0.6944364309310913, 'eval_accuracy': 0.504, 'eval_runtime': 7.9074, 'eval_samples_per_second': 63.232, 'eval_steps_per_second': 31.616, 'epoch': 2.0}\n",
      "Attempted to log scalar metric train_runtime:\n",
      "238.706\n",
      "Attempted to log scalar metric train_samples_per_second:\n",
      "16.757\n",
      "Attempted to log scalar metric train_steps_per_second:\n",
      "8.379\n",
      "Attempted to log scalar metric total_flos:\n",
      "665224914599760.0\n",
      "Attempted to log scalar metric train_loss:\n",
      "0.7160103454589843\n",
      "Attempted to log scalar metric epoch:\n",
      "2.0\n",
      "{'train_runtime': 238.706, 'train_samples_per_second': 16.757, 'train_steps_per_second': 8.379, 'train_loss': 0.7160103454589843, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2000, training_loss=0.7160103454589843, metrics={'train_runtime': 238.706, 'train_samples_per_second': 16.757, 'train_steps_per_second': 8.379, 'train_loss': 0.7160103454589843, 'epoch': 2.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_classifier_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reference_classifier.save_pretrained(\"data/reference_classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create LoRA Classifier\n",
    "\n",
    "unlike the reference classifier, the LoRA classifier does not get a new head but will be extended by another layer. The outputs of the new layer are contatenated with an existing layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "import numpy as np\n",
    "\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "pre_trained_model = BertForSequenceClassification.from_pretrained(PRE_TRAINED_MODEL,\n",
    "                                                                  num_labels = 2,\n",
    "                                                                  label2id = label2id,\n",
    "                                                                  id2label = id2label)\n",
    "\n",
    "# use std. settings for LoRA\n",
    "lora_classifier = get_peft_model(pre_trained_model, LoraConfig(task_type=\"SEQ_CLS\",inference_mode=False))\n",
    "lora_classifier.to(\"cuda\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "lora_classifier_trainer = create_trainer(lora_classifier, \"data/lora_classifier_\", train_data_ethics, test_data_ethics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate LoRA Classifier prior to training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempted to log scalar metric eval_loss:\n",
      "0.7027692794799805\n",
      "Attempted to log scalar metric eval_accuracy:\n",
      "0.496\n",
      "Attempted to log scalar metric eval_runtime:\n",
      "8.3712\n",
      "Attempted to log scalar metric eval_samples_per_second:\n",
      "59.728\n",
      "Attempted to log scalar metric eval_steps_per_second:\n",
      "29.864\n",
      "{'eval_loss': 0.7027692794799805, 'eval_accuracy': 0.496, 'eval_runtime': 8.3712, 'eval_samples_per_second': 59.728, 'eval_steps_per_second': 29.864}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.7027692794799805,\n",
       " 'eval_accuracy': 0.496,\n",
       " 'eval_runtime': 8.3712,\n",
       " 'eval_samples_per_second': 59.728,\n",
       " 'eval_steps_per_second': 29.864}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note to reviewer: the LoRA layer is not yet intiliazed. It is therefore not unclear to me if the evaluation makes sense.\n",
    "# This was the reason to create a basic classifier for the comparison \n",
    "# (that was trained on a different dataset but had initialized weights).\n",
    "# However, kindly find the pre-training evaluation results below.\n",
    "lora_classifier_trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tune LoRA Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempted to log scalar metric loss:\n",
      "0.6952\n",
      "Attempted to log scalar metric grad_norm:\n",
      "3.5249273777008057\n",
      "Attempted to log scalar metric learning_rate:\n",
      "3.7500000000000003e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "0.5\n",
      "{'loss': 0.6952, 'grad_norm': 3.5249273777008057, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.5}\n",
      "Attempted to log scalar metric loss:\n",
      "0.694\n",
      "Attempted to log scalar metric grad_norm:\n",
      "3.485154628753662\n",
      "Attempted to log scalar metric learning_rate:\n",
      "2.5e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "1.0\n",
      "{'loss': 0.694, 'grad_norm': 3.485154628753662, 'learning_rate': 2.5e-05, 'epoch': 1.0}\n",
      "Attempted to log scalar metric eval_loss:\n",
      "0.6974830031394958\n",
      "Attempted to log scalar metric eval_accuracy:\n",
      "0.504\n",
      "Attempted to log scalar metric eval_runtime:\n",
      "8.3696\n",
      "Attempted to log scalar metric eval_samples_per_second:\n",
      "59.74\n",
      "Attempted to log scalar metric eval_steps_per_second:\n",
      "29.87\n",
      "Attempted to log scalar metric epoch:\n",
      "1.0\n",
      "{'eval_loss': 0.6974830031394958, 'eval_accuracy': 0.504, 'eval_runtime': 8.3696, 'eval_samples_per_second': 59.74, 'eval_steps_per_second': 29.87, 'epoch': 1.0}\n",
      "Attempted to log scalar metric loss:\n",
      "0.6966\n",
      "Attempted to log scalar metric grad_norm:\n",
      "8.696250915527344\n",
      "Attempted to log scalar metric learning_rate:\n",
      "1.25e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "1.5\n",
      "{'loss': 0.6966, 'grad_norm': 8.696250915527344, 'learning_rate': 1.25e-05, 'epoch': 1.5}\n",
      "Attempted to log scalar metric loss:\n",
      "0.6984\n",
      "Attempted to log scalar metric grad_norm:\n",
      "4.136983871459961\n",
      "Attempted to log scalar metric learning_rate:\n",
      "0.0\n",
      "Attempted to log scalar metric epoch:\n",
      "2.0\n",
      "{'loss': 0.6984, 'grad_norm': 4.136983871459961, 'learning_rate': 0.0, 'epoch': 2.0}\n",
      "Attempted to log scalar metric eval_loss:\n",
      "0.6949747204780579\n",
      "Attempted to log scalar metric eval_accuracy:\n",
      "0.504\n",
      "Attempted to log scalar metric eval_runtime:\n",
      "8.3105\n",
      "Attempted to log scalar metric eval_samples_per_second:\n",
      "60.165\n",
      "Attempted to log scalar metric eval_steps_per_second:\n",
      "30.082\n",
      "Attempted to log scalar metric epoch:\n",
      "2.0\n",
      "{'eval_loss': 0.6949747204780579, 'eval_accuracy': 0.504, 'eval_runtime': 8.3105, 'eval_samples_per_second': 60.165, 'eval_steps_per_second': 30.082, 'epoch': 2.0}\n",
      "Attempted to log scalar metric train_runtime:\n",
      "171.7208\n",
      "Attempted to log scalar metric train_samples_per_second:\n",
      "23.294\n",
      "Attempted to log scalar metric train_steps_per_second:\n",
      "11.647\n",
      "Attempted to log scalar metric total_flos:\n",
      "667527427520160.0\n",
      "Attempted to log scalar metric train_loss:\n",
      "0.6960411224365234\n",
      "Attempted to log scalar metric epoch:\n",
      "2.0\n",
      "{'train_runtime': 171.7208, 'train_samples_per_second': 23.294, 'train_steps_per_second': 11.647, 'train_loss': 0.6960411224365234, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2000, training_loss=0.6960411224365234, metrics={'train_runtime': 171.7208, 'train_samples_per_second': 23.294, 'train_steps_per_second': 11.647, 'train_loss': 0.6960411224365234, 'epoch': 2.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_classifier_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_classifier.save_pretrained(\"data/lora_classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Classifiers after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# load\n",
    "lora_classifier = BertForSequenceClassification.from_pretrained(\"data/lora_classifier\",\n",
    "                                                                num_labels = 2,\n",
    "                                                                label2id = label2id,\n",
    "                                                                id2label = id2label)\n",
    "lora_classifier.to(\"cuda\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempted to log scalar metric eval_loss:\n",
      "0.6944364309310913\n",
      "Attempted to log scalar metric eval_accuracy:\n",
      "0.504\n",
      "Attempted to log scalar metric eval_runtime:\n",
      "8.0635\n",
      "Attempted to log scalar metric eval_samples_per_second:\n",
      "62.008\n",
      "Attempted to log scalar metric eval_steps_per_second:\n",
      "31.004\n",
      "Attempted to log scalar metric epoch:\n",
      "2.0\n",
      "{'eval_loss': 0.6944364309310913, 'eval_accuracy': 0.504, 'eval_runtime': 8.0635, 'eval_samples_per_second': 62.008, 'eval_steps_per_second': 31.004, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6944364309310913,\n",
       " 'eval_accuracy': 0.504,\n",
       " 'eval_runtime': 8.0635,\n",
       " 'eval_samples_per_second': 62.008,\n",
       " 'eval_steps_per_second': 31.004,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_classifier_trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempted to log scalar metric eval_loss:\n",
      "0.6949747204780579\n",
      "Attempted to log scalar metric eval_accuracy:\n",
      "0.504\n",
      "Attempted to log scalar metric eval_runtime:\n",
      "8.3497\n",
      "Attempted to log scalar metric eval_samples_per_second:\n",
      "59.882\n",
      "Attempted to log scalar metric eval_steps_per_second:\n",
      "29.941\n",
      "Attempted to log scalar metric epoch:\n",
      "2.0\n",
      "{'eval_loss': 0.6949747204780579, 'eval_accuracy': 0.504, 'eval_runtime': 8.3497, 'eval_samples_per_second': 59.882, 'eval_steps_per_second': 29.941, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6949747204780579,\n",
       " 'eval_accuracy': 0.504,\n",
       " 'eval_runtime': 8.3497,\n",
       " 'eval_samples_per_second': 59.882,\n",
       " 'eval_steps_per_second': 29.941,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_classifier_trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## => training improved the LoRA Classifier performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Cross Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classify_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe woman left the house\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mclassify_text\u001b[49m(text, reference_classifier)\n\u001b[1;32m      4\u001b[0m classify_text(text, lora_classifier)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'classify_text' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "text = \"The woman left the house\"\n",
    "classify_text(text, reference_classifier)\n",
    "classify_text(text, lora_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "we can see that the evaluations for both models are identical. Same applies for the manual cross check. This implies that the training data provided was not enough to \"transfer\" the exisiting classifier to a new task.\n",
    "\n",
    "The reason for selecting a very small subset for the training / evaluation task are the limited GPU resouces available.\n",
    "\n",
    "This shows that even with very poweful and well trained models, a major success for transfer learning is related to the data available.\n",
    "\n",
    "More experiments on more powerful GPUs are mandatory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
