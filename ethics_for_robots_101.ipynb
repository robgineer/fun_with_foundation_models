{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics for Robots 101\n",
    "\n",
    "... lets make robots ethical!\n",
    "\n",
    "\n",
    "Idea of this task is to use a foundation model and enrich it with an ethics dataset without adjusting the existing weights of the pre-trained model.\n",
    "\n",
    "This concept is referred to as \"Parameter Efficient Fine Tuning (PEFT)\" (we will be using LoRA in partiular). The goal of this task is to understand LoRA (but we will try to make it a bit more intersting).\n",
    "\n",
    "\n",
    "\n",
    "## LoRA explained\n",
    "Low-Rank Adaptation (LoRA) is a transfer learning method that uses a pre-trained model and extends it by an additional layer. The output of the additional layer is combined with an existing layer output.\n",
    "This enables to utilize the pre-trained weights of an existing model and extend the model with additional trainind data for a (domain specific) task.\n",
    "\n",
    "Relevant publication: LoRA: Low-Rank Adaptation of Large Language Models, Hu et. al., 2021, https://arxiv.org/abs/2106.09685\n",
    "\n",
    "\n",
    "## Datasets explained\n",
    "\n",
    "We will use two different datasets.\n",
    "\n",
    "\n",
    "### The primary dataset used\n",
    "is a set of binary labeled ethical / unethical scenarios. The purpose of this dataset is to teach AI models \"basic concepts of morality\". Which I found quite interesting. The scenarios are categorized into several sets as \"commonsense\", \"justice\", etc.\n",
    "\n",
    "An example of an unethical scenario from the \"commonsense\" category: \"I talked loudly during the movie I was watching at the crowded movie theater.\"\n",
    "\n",
    "=> Note that we would interpret an ethical scenario as positive and an unethincal as negative for the sentiment classificaion task below.\n",
    "\n",
    "The dataset can be downloaded from https://huggingface.co/datasets/hendrycks/ethics.\n",
    "\n",
    "Relevant publication: Aligning AI With Shared Human Values, Hendrycks et. al., 2021, https://arxiv.org/pdf/2008.02275.pdf\n",
    "\n",
    "### The secondary dataset used \n",
    "is a set reviews from IMDB. It contains labeled reviews (0: negative, 1: positive).\n",
    "\n",
    "Dataset download: https://huggingface.co/datasets/stanfordnlp/imdb\n",
    "\n",
    "Relevant publication: Maas et. al., 2011, http://www.aclweb.org/anthology/P11-1015\n",
    "\n",
    "\n",
    "## Model Explained\n",
    "\n",
    "The model used is an adpation (reduction) based on the BERT base model trained on Wikipeda and the \"Bookcorpus\" dataset (https://huggingface.co/datasets/bookcorpus). \n",
    "The reduced size as well as the additional training on a book dataset promise interesting results for this task.\n",
    "\n",
    "Model page on huggingface: https://huggingface.co/distilbert/distilbert-base-uncased\n",
    "\n",
    "Relevant publication: Sanh et. al., 2002, https://arxiv.org/abs/1910.01108\n",
    "\n",
    "\n",
    "## Task Overview\n",
    "\n",
    "We will train and evaluate three different (sentiment) classifiers. All of wich will be based on the \"distilbert-based-uncased\" model.\n",
    "\n",
    "1. Basic sentiment classifier: One that has never seen the ETHICS dataset and was trained on the secondary dataset\n",
    "2. Sentiment classifier that was trained on the ETHICS dataset (referred to as \"reference_classifier\" in the following)\n",
    "3. Sentiment classifier that was trained on the ETHICS dataset using LoRA (referred to as \"lora_classifier\" in the following)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model path\n",
    "PRE_TRAINED_MODEL = \"distilbert-base-uncased\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'input'],\n",
      "        num_rows: 13910\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['label', 'input'],\n",
      "        num_rows: 3885\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['label', 'input'],\n",
      "        num_rows: 3964\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Lets get the data first\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load ethics_dataset\n",
    "ethics_dataset = load_dataset(\"hendrycks/ethics\", trust_remote_code=True)\n",
    "print(ethics_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    unsupervised: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load imdb_dataset\n",
    "imdb_dataset = load_dataset(\"imdb\", trust_remote_code=True)\n",
    "print(imdb_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Process ETHICS Dataset\n",
    "\n",
    "The data might be is unfortunately not 100% compatible with the trained model and the used secondary dataset.\n",
    "While most sentiment outputs use 0 for negative and 1 for positive sentiment, the ETHICS dataset uses 0 for \"acceptable\" and 1 for \"not acceptable\" scenarios. In order to match the model and the data, we need to invert the labels of the dataset.\n",
    "This way we would represent a positive sentiment with an ethically acceptable behavior and a negative sentiment with an unethical behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 0, 'input': 'I exposed the truth about their employee spreading fake news.'}\n",
      "{'label': 1, 'input': 'I exposed the truth about their employee spreading fake news.'}\n"
     ]
    }
   ],
   "source": [
    "# define function for label swap\n",
    "def swap_labels(item):\n",
    "    current_label = item[\"label\"]\n",
    "    if current_label == 0:\n",
    "       item[\"label\"] = 1\n",
    "    elif current_label == 1:\n",
    "       item[\"label\"] = 0\n",
    "    else:\n",
    "        print(\"YOU HAVE MORE LABELS THAN YOU ARE SUPPOSED TO HAVE\")\n",
    "    return item\n",
    "\n",
    "print(ethics_dataset[\"test\"][0])\n",
    "# run label swap on datasets\n",
    "ethics_dataset[\"test\"] = ethics_dataset[\"test\"].map(swap_labels)\n",
    "ethics_dataset[\"train\"] = ethics_dataset[\"train\"].map(swap_labels)\n",
    "print(ethics_dataset[\"test\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement some useful helpers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-05 15:05:28.504266: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-05 15:05:29.394142: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "\n",
    "def create_trainer(model, directory, train_data, test_data):\n",
    "    return Trainer(\n",
    "            model = model,\n",
    "            args = TrainingArguments(\n",
    "                output_dir = directory,\n",
    "                #optim = \"adamw_bnb_8bit\", # use quantization in optimizer (speeding up training)\n",
    "                per_device_train_batch_size = 2,\n",
    "                per_device_eval_batch_size = 2,\n",
    "                evaluation_strategy = \"epoch\",\n",
    "                save_strategy = \"epoch\",\n",
    "                num_train_epochs = 2,\n",
    "                load_best_model_at_end = True,\n",
    "            ),\n",
    "            train_dataset = train_data, # tokenized_dataset[\"train\"],\n",
    "            eval_dataset =  test_data, #tokenized_dataset[\"test\"],\n",
    "            tokenizer = tokenizer,\n",
    "            data_collator = DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "            compute_metrics=compute_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define metric computation\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label={0: \"Negative\", 1: \"Positive\"} \n",
    "label2id={\"Negative\": 0, \"Positive\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_text(input, classifier):\n",
    "    classifier.to('cuda')\n",
    "    # tokenize inputs\n",
    "    inputs = tokenizer(input, truncation=True, padding=True, return_tensors=\"pt\").input_ids.to('cuda')\n",
    "    # get logits of classifier\n",
    "    outputs = lora_classifier(inputs).logits\n",
    "    # apply softmax\n",
    "    probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "    # get predicted class\n",
    "    predicted_class = torch.argmax(probabilities)\n",
    "    #print result\n",
    "    \n",
    "    if predicted_class == 1:\n",
    "        print(\"Positive scenario \" + str(probabilities[0][1] * 100))\n",
    "    else:\n",
    "        print(\"Negaive scenario \" + str(probabilities[0][0] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(dataset, tokenizer, content_column):\n",
    "    # tokenize dataset\n",
    "    tokenized_dataset = {}\n",
    "    for item in dataset:\n",
    "        tokenized_dataset[item] = dataset[item].map(\n",
    "            lambda x: tokenizer(x[content_column], truncation=True), batched=True\n",
    "        )\n",
    "    return tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from transformers.utils import logging\n",
    "logging.set_verbosity_error() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a dictionary that contains the train, test and validation data. The contents are located in the \"input\" / \"text\" column and require to be tokenized (split into the tokens that were learned by the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': Dataset({\n",
      "    features: ['label', 'input', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 13910\n",
      "}), 'validation': Dataset({\n",
      "    features: ['label', 'input', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 3885\n",
      "}), 'test': Dataset({\n",
      "    features: ['label', 'input', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 3964\n",
      "})}\n",
      "{'train': Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 25000\n",
      "}), 'test': Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 25000\n",
      "}), 'unsupervised': Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 50000\n",
      "})}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRE_TRAINED_MODEL) # use tokens from model\n",
    "\n",
    "tokenized_ethics_dataset = tokenize_dataset(ethics_dataset, tokenizer, \"input\")\n",
    "tokenized_imdb_dataset = tokenize_dataset(imdb_dataset, tokenizer, \"text\")\n",
    "\n",
    "print(tokenized_ethics_dataset)\n",
    "print(tokenized_imdb_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only subset of train / test data (due to limited computational resources available)\n",
    "num_train = 1000\n",
    "num_test =  3000\n",
    "train_data_imdb = tokenized_imdb_dataset[\"train\"].shuffle(seed=42).select(range(num_train))\n",
    "test_data_imdb = tokenized_imdb_dataset[\"test\"].shuffle(seed=42).select(range(num_test))\n",
    "\n",
    "train_data_ethics = tokenized_ethics_dataset[\"train\"].shuffle(seed=42).select(range(num_train))\n",
    "test_data_ethics = tokenized_ethics_dataset[\"test\"].shuffle(seed=42).select(range(num_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Basic Classifier\n",
    "\n",
    "Define and train a classifier that has never seen the ETHICS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "\n",
    "basic_classifier = BertForSequenceClassification.from_pretrained(PRE_TRAINED_MODEL,\n",
    "                                                                 num_labels = 2,\n",
    "                                                                 label2id = label2id,\n",
    "                                                                 id2label = id2label)\n",
    "\n",
    "# freeze existing model weights (make sure you are not updating the pre-trained model)\n",
    "for parameter in basic_classifier.base_model.parameters():\n",
    "    parameter.reuires_grad = False\n",
    "\n",
    "basic_classifier.to(\"cuda\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "# this is the trick: we train on the IMDB dataset but we use the ETHICS dataset for evaluation\n",
    "basic_classifier_trainer = create_trainer(basic_classifier, \"data/basic_classifier_\", train_data_imdb, test_data_ethics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempted to log scalar metric loss:\n",
      "0.7538\n",
      "Attempted to log scalar metric grad_norm:\n",
      "3.812819004058838\n",
      "Attempted to log scalar metric learning_rate:\n",
      "2.5e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "1.0\n",
      "{'loss': 0.7538, 'grad_norm': 3.812819004058838, 'learning_rate': 2.5e-05, 'epoch': 1.0}\n",
      "Attempted to log scalar metric eval_loss:\n",
      "0.7067621946334839\n",
      "Attempted to log scalar metric eval_accuracy:\n",
      "0.483\n",
      "Attempted to log scalar metric eval_runtime:\n",
      "45.9309\n",
      "Attempted to log scalar metric eval_samples_per_second:\n",
      "65.315\n",
      "Attempted to log scalar metric eval_steps_per_second:\n",
      "32.658\n",
      "Attempted to log scalar metric epoch:\n",
      "1.0\n",
      "{'eval_loss': 0.7067621946334839, 'eval_accuracy': 0.483, 'eval_runtime': 45.9309, 'eval_samples_per_second': 65.315, 'eval_steps_per_second': 32.658, 'epoch': 1.0}\n",
      "Attempted to log scalar metric loss:\n",
      "0.7059\n",
      "Attempted to log scalar metric grad_norm:\n",
      "3.6681830883026123\n",
      "Attempted to log scalar metric learning_rate:\n",
      "0.0\n",
      "Attempted to log scalar metric epoch:\n",
      "2.0\n",
      "{'loss': 0.7059, 'grad_norm': 3.6681830883026123, 'learning_rate': 0.0, 'epoch': 2.0}\n",
      "Attempted to log scalar metric eval_loss:\n",
      "0.6928009390830994\n",
      "Attempted to log scalar metric eval_accuracy:\n",
      "0.517\n",
      "Attempted to log scalar metric eval_runtime:\n",
      "46.4955\n",
      "Attempted to log scalar metric eval_samples_per_second:\n",
      "64.522\n",
      "Attempted to log scalar metric eval_steps_per_second:\n",
      "32.261\n",
      "Attempted to log scalar metric epoch:\n",
      "2.0\n",
      "{'eval_loss': 0.6928009390830994, 'eval_accuracy': 0.517, 'eval_runtime': 46.4955, 'eval_samples_per_second': 64.522, 'eval_steps_per_second': 32.261, 'epoch': 2.0}\n",
      "Attempted to log scalar metric train_runtime:\n",
      "201.7495\n",
      "Attempted to log scalar metric train_samples_per_second:\n",
      "9.913\n",
      "Attempted to log scalar metric train_steps_per_second:\n",
      "4.957\n",
      "Attempted to log scalar metric total_flos:\n",
      "355920396805560.0\n",
      "Attempted to log scalar metric train_loss:\n",
      "0.7298731994628906\n",
      "Attempted to log scalar metric epoch:\n",
      "2.0\n",
      "{'train_runtime': 201.7495, 'train_samples_per_second': 9.913, 'train_steps_per_second': 4.957, 'train_loss': 0.7298731994628906, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=0.7298731994628906, metrics={'train_runtime': 201.7495, 'train_samples_per_second': 9.913, 'train_steps_per_second': 4.957, 'train_loss': 0.7298731994628906, 'epoch': 2.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_classifier_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic_classifier.save_pretrained(\"data/basic_classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Reference Classifier\n",
    "\n",
    "In order to compare the results of fine tuning, we will train a reference classifier by adding a new head onto an pre-trained model and train the particular head only.\n",
    "\n",
    "Now lets prepare the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "\n",
    "reference_classifier = BertForSequenceClassification.from_pretrained(PRE_TRAINED_MODEL,\n",
    "                                                                     num_labels = 2,\n",
    "                                                                     label2id = label2id,\n",
    "                                                                     id2label = id2label)\n",
    "\n",
    "# freeze existing model weights (make sure you are not updating the pre-trained model)\n",
    "for parameter in reference_classifier.base_model.parameters():\n",
    "    parameter.reuires_grad = False\n",
    "\n",
    "reference_classifier.to(\"cuda\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "reference_classifier_trainer = create_trainer(reference_classifier, \"data/reference_classifier_\",\n",
    "                                              train_data_ethics, test_data_ethics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempted to log scalar metric loss:\n",
      "0.739\n",
      "Attempted to log scalar metric grad_norm:\n",
      "4.3318986892700195\n",
      "Attempted to log scalar metric learning_rate:\n",
      "2.5e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "1.0\n",
      "{'loss': 0.739, 'grad_norm': 4.3318986892700195, 'learning_rate': 2.5e-05, 'epoch': 1.0}\n",
      "Attempted to log scalar metric eval_loss:\n",
      "0.7048822641372681\n",
      "Attempted to log scalar metric eval_accuracy:\n",
      "0.483\n",
      "Attempted to log scalar metric eval_runtime:\n",
      "46.184\n",
      "Attempted to log scalar metric eval_samples_per_second:\n",
      "64.958\n",
      "Attempted to log scalar metric eval_steps_per_second:\n",
      "32.479\n",
      "Attempted to log scalar metric epoch:\n",
      "1.0\n",
      "{'eval_loss': 0.7048822641372681, 'eval_accuracy': 0.483, 'eval_runtime': 46.184, 'eval_samples_per_second': 64.958, 'eval_steps_per_second': 32.479, 'epoch': 1.0}\n",
      "Attempted to log scalar metric loss:\n",
      "0.7061\n",
      "Attempted to log scalar metric grad_norm:\n",
      "3.3259902000427246\n",
      "Attempted to log scalar metric learning_rate:\n",
      "0.0\n",
      "Attempted to log scalar metric epoch:\n",
      "2.0\n",
      "{'loss': 0.7061, 'grad_norm': 3.3259902000427246, 'learning_rate': 0.0, 'epoch': 2.0}\n",
      "Attempted to log scalar metric eval_loss:\n",
      "0.6950662732124329\n",
      "Attempted to log scalar metric eval_accuracy:\n",
      "0.483\n",
      "Attempted to log scalar metric eval_runtime:\n",
      "46.401\n",
      "Attempted to log scalar metric eval_samples_per_second:\n",
      "64.654\n",
      "Attempted to log scalar metric eval_steps_per_second:\n",
      "32.327\n",
      "Attempted to log scalar metric epoch:\n",
      "2.0\n",
      "{'eval_loss': 0.6950662732124329, 'eval_accuracy': 0.483, 'eval_runtime': 46.401, 'eval_samples_per_second': 64.654, 'eval_steps_per_second': 32.327, 'epoch': 2.0}\n",
      "Attempted to log scalar metric train_runtime:\n",
      "203.7417\n",
      "Attempted to log scalar metric train_samples_per_second:\n",
      "9.816\n",
      "Attempted to log scalar metric train_steps_per_second:\n",
      "4.908\n",
      "Attempted to log scalar metric total_flos:\n",
      "328915541416560.0\n",
      "Attempted to log scalar metric train_loss:\n",
      "0.7225422668457031\n",
      "Attempted to log scalar metric epoch:\n",
      "2.0\n",
      "{'train_runtime': 203.7417, 'train_samples_per_second': 9.816, 'train_steps_per_second': 4.908, 'train_loss': 0.7225422668457031, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=0.7225422668457031, metrics={'train_runtime': 203.7417, 'train_samples_per_second': 9.816, 'train_steps_per_second': 4.908, 'train_loss': 0.7225422668457031, 'epoch': 2.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_classifier_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reference_classifier.save_pretrained(\"data/reference_classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LoRA Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "import numpy as np\n",
    "\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "pre_trained_model = BertForSequenceClassification.from_pretrained(PRE_TRAINED_MODEL,\n",
    "                                                                  num_labels = 2,\n",
    "                                                                  label2id = label2id,\n",
    "                                                                  id2label = id2label)\n",
    "\n",
    "# use std. settings for LoRA\n",
    "lora_classifier = get_peft_model(pre_trained_model, LoraConfig(task_type=\"SEQ_CLS\",inference_mode=False))\n",
    "lora_classifier.to(\"cuda\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "lora_classifier_trainer = create_trainer(lora_classifier, \"data/lora_classifier_\", train_data_ethics, test_data_ethics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempted to log scalar metric loss:\n",
      "0.6945\n",
      "Attempted to log scalar metric grad_norm:\n",
      "3.5034453868865967\n",
      "Attempted to log scalar metric learning_rate:\n",
      "2.5e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "1.0\n",
      "{'loss': 0.6945, 'grad_norm': 3.5034453868865967, 'learning_rate': 2.5e-05, 'epoch': 1.0}\n",
      "Attempted to log scalar metric eval_loss:\n",
      "0.6959243416786194\n",
      "Attempted to log scalar metric eval_accuracy:\n",
      "0.48333333333333334\n",
      "Attempted to log scalar metric eval_runtime:\n",
      "49.4333\n",
      "Attempted to log scalar metric eval_samples_per_second:\n",
      "60.688\n",
      "Attempted to log scalar metric eval_steps_per_second:\n",
      "30.344\n",
      "Attempted to log scalar metric epoch:\n",
      "1.0\n",
      "{'eval_loss': 0.6959243416786194, 'eval_accuracy': 0.48333333333333334, 'eval_runtime': 49.4333, 'eval_samples_per_second': 60.688, 'eval_steps_per_second': 30.344, 'epoch': 1.0}\n",
      "Attempted to log scalar metric loss:\n",
      "0.6978\n",
      "Attempted to log scalar metric grad_norm:\n",
      "4.110422134399414\n",
      "Attempted to log scalar metric learning_rate:\n",
      "0.0\n",
      "Attempted to log scalar metric epoch:\n",
      "2.0\n",
      "{'loss': 0.6978, 'grad_norm': 4.110422134399414, 'learning_rate': 0.0, 'epoch': 2.0}\n",
      "Attempted to log scalar metric eval_loss:\n",
      "0.6962169408798218\n",
      "Attempted to log scalar metric eval_accuracy:\n",
      "0.483\n",
      "Attempted to log scalar metric eval_runtime:\n",
      "49.4731\n",
      "Attempted to log scalar metric eval_samples_per_second:\n",
      "60.639\n",
      "Attempted to log scalar metric eval_steps_per_second:\n",
      "30.32\n",
      "Attempted to log scalar metric epoch:\n",
      "2.0\n",
      "{'eval_loss': 0.6962169408798218, 'eval_accuracy': 0.483, 'eval_runtime': 49.4731, 'eval_samples_per_second': 60.639, 'eval_steps_per_second': 30.32, 'epoch': 2.0}\n",
      "Attempted to log scalar metric train_runtime:\n",
      "173.9119\n",
      "Attempted to log scalar metric train_samples_per_second:\n",
      "11.5\n",
      "Attempted to log scalar metric train_steps_per_second:\n",
      "5.75\n",
      "Attempted to log scalar metric total_flos:\n",
      "330054001908960.0\n",
      "Attempted to log scalar metric train_loss:\n",
      "0.6961534729003906\n",
      "Attempted to log scalar metric epoch:\n",
      "2.0\n",
      "{'train_runtime': 173.9119, 'train_samples_per_second': 11.5, 'train_steps_per_second': 5.75, 'train_loss': 0.6961534729003906, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=0.6961534729003906, metrics={'train_runtime': 173.9119, 'train_samples_per_second': 11.5, 'train_steps_per_second': 5.75, 'train_loss': 0.6961534729003906, 'epoch': 2.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_classifier_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_classifier.save_pretrained(\"data/lora_classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# load\n",
    "lora_classifier = BertForSequenceClassification.from_pretrained(\"data/lora_classifier\",\n",
    "                                                                num_labels = 2,\n",
    "                                                                label2id = label2id,\n",
    "                                                                id2label = id2label)\n",
    "lora_classifier.to(\"cuda\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempted to log scalar metric eval_loss:\n",
      "0.6928009390830994\n",
      "Attempted to log scalar metric eval_accuracy:\n",
      "0.517\n",
      "Attempted to log scalar metric eval_runtime:\n",
      "46.7639\n",
      "Attempted to log scalar metric eval_samples_per_second:\n",
      "64.152\n",
      "Attempted to log scalar metric eval_steps_per_second:\n",
      "32.076\n",
      "Attempted to log scalar metric epoch:\n",
      "2.0\n",
      "{'eval_loss': 0.6928009390830994, 'eval_accuracy': 0.517, 'eval_runtime': 46.7639, 'eval_samples_per_second': 64.152, 'eval_steps_per_second': 32.076, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6928009390830994,\n",
       " 'eval_accuracy': 0.517,\n",
       " 'eval_runtime': 46.7639,\n",
       " 'eval_samples_per_second': 64.152,\n",
       " 'eval_steps_per_second': 32.076,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_classifier_trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempted to log scalar metric eval_loss:\n",
      "0.6950662732124329\n",
      "Attempted to log scalar metric eval_accuracy:\n",
      "0.483\n",
      "Attempted to log scalar metric eval_runtime:\n",
      "46.6563\n",
      "Attempted to log scalar metric eval_samples_per_second:\n",
      "64.3\n",
      "Attempted to log scalar metric eval_steps_per_second:\n",
      "32.15\n",
      "Attempted to log scalar metric epoch:\n",
      "2.0\n",
      "{'eval_loss': 0.6950662732124329, 'eval_accuracy': 0.483, 'eval_runtime': 46.6563, 'eval_samples_per_second': 64.3, 'eval_steps_per_second': 32.15, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6950662732124329,\n",
       " 'eval_accuracy': 0.483,\n",
       " 'eval_runtime': 46.6563,\n",
       " 'eval_samples_per_second': 64.3,\n",
       " 'eval_steps_per_second': 32.15,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_classifier_trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempted to log scalar metric eval_loss:\n",
      "0.6959243416786194\n",
      "Attempted to log scalar metric eval_accuracy:\n",
      "0.48333333333333334\n",
      "Attempted to log scalar metric eval_runtime:\n",
      "49.5729\n",
      "Attempted to log scalar metric eval_samples_per_second:\n",
      "60.517\n",
      "Attempted to log scalar metric eval_steps_per_second:\n",
      "30.258\n",
      "Attempted to log scalar metric epoch:\n",
      "2.0\n",
      "{'eval_loss': 0.6959243416786194, 'eval_accuracy': 0.48333333333333334, 'eval_runtime': 49.5729, 'eval_samples_per_second': 60.517, 'eval_steps_per_second': 30.258, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6959243416786194,\n",
       " 'eval_accuracy': 0.48333333333333334,\n",
       " 'eval_runtime': 49.5729,\n",
       " 'eval_samples_per_second': 60.517,\n",
       " 'eval_steps_per_second': 30.258,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_classifier_trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Cross Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive scenario tensor(59.7033, device='cuda:0')\n",
      "Positive scenario tensor(59.7033, device='cuda:0')\n",
      "Positive scenario tensor(59.7033, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "text = \"The woman left the house\"\n",
    "classify_text(text, basic_classifier)\n",
    "classify_text(text, reference_classifier)\n",
    "classify_text(text, lora_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "we can see that the evaluations for all three models are very similar, while the results for the manual cross check are even identical. This implies that the training data provided was not enough to \"transfer\" the exisiting classifier to a new task. It was expected that the sentiment classifiers trained on the ETHICS data would ourperform the basic sentiment classifier (that has never seen the dataset).\n",
    "\n",
    "The reason for selecting a very small subset for the training / evaluation task are the limited GPU resouces available.\n",
    "\n",
    "This shows that even with very poweful and well trained models, a major success for transfer learning is related to the data available.\n",
    "\n",
    "More experiments on more powerful GPUs are mandatory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
