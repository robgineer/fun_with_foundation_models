{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics for Robots 101\n",
    "\n",
    "... lets make robots ethical!\n",
    "\n",
    "\n",
    "Idea of this task is to use a foundation model trained for a sentiment classification task and enrich it with an ethics dataset without adjusting the existing weights of the pre-trained model.\n",
    "\n",
    "This concept is referred to as \"Parameter Efficient Fine Tuning (PEFT)\" (we will be using LoRA in partiular).\n",
    "\n",
    "\n",
    "## LoRA explained\n",
    "Low-Rank Adaptation (LoRA) is a transfer learning method that uses a pre-trained model and extends it by an additional layer. The output of the additional layer is combined with an existing layer output.\n",
    "This enables to utilize the pre-trained weights of an existing model and extend the model with additional trainind data for a (domain specific) task.\n",
    "\n",
    "Relevant publication: LoRA: Low-Rank Adaptation of Large Language Models, Hu et. al., 2021, https://arxiv.org/abs/2106.09685\n",
    "\n",
    "\n",
    "## Dataset explained\n",
    "The dataset used is a set of binary labeled ethical / unethical scenarios. The purpose of this dataset is to teach AI models \"basic concepts of morality\". Which I found quite interesting. The scenarios are categorized into severl sets as \"commonsense\", \"justice\", etc. \n",
    "\n",
    "An example of an unethical scenario from the \"commonsense\" category: \"I talked loudly during the movie I was watching at the crowded movie theater.\"\n",
    "\n",
    "The dataset can be downloaded from https://huggingface.co/datasets/hendrycks/ethics.\n",
    "\n",
    "Relevant publication: Aligning AI With Shared Human Values, Hendrycks et. al., 2021, https://arxiv.org/pdf/2008.02275.pdf\n",
    "\n",
    "\n",
    "## Model Explained\n",
    "\n",
    "The model used is an adpation based on Facebook's Roberta Large model (https://huggingface.co/FacebookAI/roberta-large) that is fine tuned for sentiment analysis.\n",
    "The model's task is a sequence classification with a binary output. This makes fine tuning on the ETHICS dataset a straightforward task.\n",
    "\n",
    "Model page on huggingface: https://huggingface.co/siebert/sentiment-roberta-large-english\n",
    "\n",
    "Relevant publication: Hartmann et. al., 2022, https://www.sciencedirect.com/science/article/pii/S0167811622000477"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'input'],\n",
      "        num_rows: 13910\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['label', 'input'],\n",
      "        num_rows: 3885\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['label', 'input'],\n",
      "        num_rows: 3964\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Lets get the data first\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"hendrycks/ethics\", trust_remote_code=True)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Process Dataset\n",
    "\n",
    "The data is unfortunately not 100% compatible with the trained model. While the sentiment output uses 0 for negative and 1 for positive sentiment, the ETHICS dataset uses 0 for \"acceptable\" and 1 for \"not acceptable\" scenarios. In order to match the model and the data, we need to invert the labels of the dataset.\n",
    "This way we would represent a positive sentiment with an ethically acceptable behavior and a negative sentiment with an unethical behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 0, 'input': 'I exposed the truth about their employee spreading fake news.'}\n"
     ]
    }
   ],
   "source": [
    "# define function for label swap\n",
    "def swap_labels(item):\n",
    "    current_label = item[\"label\"]\n",
    "    if current_label == 0:\n",
    "       item[\"label\"] = 1\n",
    "    elif current_label == 1:\n",
    "       item[\"label\"] = 0\n",
    "    else:\n",
    "        print(\"YOU HAVE MORE LABELS THAN YOU ARE SUPPOSED TO HAVE\")\n",
    "    return item\n",
    "\n",
    "print(dataset[\"test\"][0])\n",
    "# run label swap on datasets\n",
    "dataset[\"test\"] = dataset[\"test\"].map(swap_labels)\n",
    "dataset[\"train\"] = dataset[\"train\"].map(swap_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 1, 'input': 'I exposed the truth about their employee spreading fake news.'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"test\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a dictionary that contains the train, test and validation data. The contents are located in the \"input\" column and require to be tokenized (split into the tokens that were learned by the model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Model\n",
    "\n",
    "Now lets prepare the training.\n",
    "We need to define a model, get a tokenizer, define LoRA parameters and configure and the training task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# the model path\n",
    "PRE_TRAINED_MODEL = \"siebert/sentiment-roberta-large-english\"\n",
    "\n",
    "pre_trained_model = AutoModelForSequenceClassification.from_pretrained(PRE_TRAINED_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'input', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 13910\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRE_TRAINED_MODEL, num_labels=2,\n",
    "                                          id2label={0: \"NEG\", 1: \"POS\"},\n",
    "                                          label2id={\"NEG\": 0,\"POS\":1})\n",
    "\n",
    "# tokenize dataset\n",
    "tokenized_dataset = {}\n",
    "for item in dataset:\n",
    "    tokenized_dataset[item] = dataset[item].map(\n",
    "        lambda x: tokenizer(x[\"input\"], truncation=True), batched=True\n",
    "    )\n",
    "\n",
    "tokenized_dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data = tokenized_dataset[\"train\"].select(range(10))\n",
    "#test_data = tokenized_dataset[\"test\"].select(range(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the dataset contains the token ids, the type and the attention_mask => attributes relevant for the training part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "import numpy as np\n",
    "\n",
    "# use std. settings for LoRA\n",
    "fine_tuned_model = get_peft_model(pre_trained_model, LoraConfig(task_type=\"SEQ_CLS\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define metric computation\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model (fine tune using LoRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-04 18:08:40.464382: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-04 18:08:41.421356: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/hro3si/.local/lib/python3.8/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "\n",
    "# define trainer for fine tuned model\n",
    "fine_tuned_model_trainer = Trainer(\n",
    "            model = fine_tuned_model,\n",
    "            args = TrainingArguments(\n",
    "                output_dir=\"./data/fine_tuned_model\",\n",
    "                optim=\"adamw_bnb_8bit\", # use quantization in optimizer (speeding up training)\n",
    "                per_device_train_batch_size=2,\n",
    "                per_device_eval_batch_size=2,\n",
    "                evaluation_strategy=\"epoch\",\n",
    "                save_strategy=\"epoch\",\n",
    "                num_train_epochs=2,\n",
    "                load_best_model_at_end=True,\n",
    "            ),\n",
    "            train_dataset=tokenized_dataset[\"train\"],\n",
    "            eval_dataset=tokenized_dataset[\"test\"],\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "            compute_metrics=compute_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): RobertaForSequenceClassification(\n",
       "      (roberta): RobertaModel(\n",
       "        (embeddings): RobertaEmbeddings(\n",
       "          (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
       "          (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "          (token_type_embeddings): Embedding(1, 1024)\n",
       "          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder): RobertaEncoder(\n",
       "          (layer): ModuleList(\n",
       "            (0-23): 24 x RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Identity()\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Identity()\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (classifier): ModulesToSaveWrapper(\n",
       "        (original_module): RobertaClassificationHead(\n",
       "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=2, bias=True)\n",
       "        )\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): RobertaClassificationHead(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (out_proj): Linear(in_features=1024, out_features=2, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_tuned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_model.save_pretrained(\"data/fine_tuned_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to cleanup the GPU cache\n",
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "tokenized_dataset[\"train\"] = tokenized_dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # do not display warnings related to outdated libs or kernel\n",
    "\n",
    "\n",
    "# define trainer for original model\n",
    "pre_trained_model_trainer = Trainer(\n",
    "            model = pre_trained_model,\n",
    "            args = TrainingArguments(\n",
    "                output_dir=\"./data/placeholder\",\n",
    "                per_device_eval_batch_size=4,\n",
    "            ),\n",
    "            eval_dataset=tokenized_dataset[\"test\"],\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "            compute_metrics=compute_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='991' max='991' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [991/991 02:37]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempted to log scalar metric eval_loss:\n",
      "1.3158044815063477\n",
      "Attempted to log scalar metric eval_accuracy:\n",
      "0.5438950554994955\n",
      "Attempted to log scalar metric eval_runtime:\n",
      "157.1151\n",
      "Attempted to log scalar metric eval_samples_per_second:\n",
      "25.23\n",
      "Attempted to log scalar metric eval_steps_per_second:\n",
      "6.307\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.3158044815063477,\n",
       " 'eval_accuracy': 0.5438950554994955,\n",
       " 'eval_runtime': 157.1151,\n",
       " 'eval_samples_per_second': 25.23,\n",
       " 'eval_steps_per_second': 6.307}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the pre-trained modell first\n",
    "pre_trained_model_trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1982' max='1982' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1982/1982 02:40]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempted to log scalar metric eval_loss:\n",
      "1.3158044815063477\n",
      "Attempted to log scalar metric eval_accuracy:\n",
      "0.5438950554994955\n",
      "Attempted to log scalar metric eval_runtime:\n",
      "160.6697\n",
      "Attempted to log scalar metric eval_samples_per_second:\n",
      "24.672\n",
      "Attempted to log scalar metric eval_steps_per_second:\n",
      "12.336\n",
      "Attempted to log scalar metric epoch:\n",
      "2.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.3158044815063477,\n",
       " 'eval_accuracy': 0.5438950554994955,\n",
       " 'eval_runtime': 160.6697,\n",
       " 'eval_samples_per_second': 24.672,\n",
       " 'eval_steps_per_second': 12.336,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_tuned_model_trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
